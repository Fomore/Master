\section{Zusammenfassung}
Für die Analyse der Gesichter in einem Video werden zuerst die einzelnen Gesichter mittels MTCNN-Face Detection (\autoref{MTCNN}) gesucht. Die Versuche betätigen, das MTCNN robust genug ist, um auch kleinste Gesichter im Bild zu erkennen. Es konnten Gesichter auf einer Distanz von $14m$ und auch deutlich abgewandte gefunden werden.\\
Anschließend wird jede Einzelperson unterschieden und alle gefundenen Bildbereiche der jeweiligen Person zugeordnet. Diese Bildbereiche werden nun auf eine Mindestgröße gebracht (\autoref{scale_Algos}), damit sie dem Trainingsdatensatz des nächsten Schrittes stärker ähneln. Dazu wurde die Auswirkung der verschieden Skalierungsverfahren auf die nachfolgende Analyse untersucht und das lineare-Verfahren als das brauchbarste identifiziert.\\
Nun werden die einzelnen Bildbereiche ausgewertet (\autoref{OpenFace}) und die Gesichtsorientierung kann bestimmt werden. Um die Bereiche zu simulieren in denen das Verfahren eingesetzt werden kann, wurden die Bilder des Trainingsdatensatzes durch lineare Skalierung verkleinert.\\
Um die Detektion der Pupille zu verbessern wurde ElSe (\autoref{ElSe}) angewendet. Um dieses Verfahren zu optimieren, wurde die Auswirkung der verschiedenen Farbbild-zu-Graubild Konvertierungsverfahren (\autoref{Graubild}) untersucht, sowie die Veränderung des Radius der Maske und die Stabilität der Ergebnisse auf linear verkleinerten Eingabebildern. Die Messung hat ergeben, dass eine Verbesserung durch ElSe vor allem bei sehr kleinen Bildern möglich ist, der Augendetektor von OpenFace allerdings auch bereits sehr gute Ergebnisse liefert.\\
Um eine Übersicht über den maximalen Arbeitsbereich der Verfahren zu erhalten, wurden verschiedene Versuche durchgeführt um die maximalen Kopfrotationen und Distanzen zu bestimmen. Dabei konnte gezeigt werden, dass der Wertebereich in dem eine Auswertung möglich ist, ausreicht um das gesamte Klassenzimmer mit nur einer Kamera zu erfassen. Dabei zeigte sich, dass gerade die Bestimmung der Blickrichtung auf großer Distanz meist nicht möglich ist, da die Augenpartie viel zu klein für eine Berechnung ist. Die Tests haben gezeigt das es theoretisch möglich ist, bis auf einer Distanz von $4m$, die Blickrichtung bestimmen. Die Versuche unter realen Bedingungen widerlegten allerdings diese Behauptung. So bleibt meist nur die Gesichtsorientierung mit ihr natürlichen Ungenauigkeit.\\
Abschließend wurde getestet, wie zuverlässig das gesamte Verfahren auf Videos unter realen Bedingungen eingesetzt werden kann, siehe \autoref{VideoAnalyse}. Dazu wurde ein Versuch durchgeführt, bei dem die Probanden ein Ziel verfolgen sollten und ermittelt wie exakt das Ziel der Aufmerksamkeit bestimmt werden konnte. Es zeigte sich, dass bereits in einer Entfernung von $1,5m$ zur Kamera eine Auswertung der Blickrichtung mit Fehlern von mehr als $17^\circ$ behaftet ist. Wird statt der Augenbewegungen nur die Kopfbewegung ausgewertet, so ist bei größeren Blickwinkeln natürlich mit einem hohen Fehler zu rechnen.\\
Da sich die Probanden frei im Raum bewegen können kommt es zudem immer wieder vor, dass Teile des Gesichtes verdeckt werden, durch Hände beim Melden, andere Schüler oder den Lehrer selbst, der vor der Kamera steht oder sich der Kopf zu weit wegdreht und das Tracking scheitert. Aber auch die Frisuren spielen eine Rolle, da durch diese vor allem die Augenbrauen verdeckt werden, welche wichtige und hoch gewichtete Landmarken bei der Lokalisierung des Gesichts sind und das Gesicht dadurch nicht erkannt wird. Eine Problematik die schon in den Versuchen aufgetreten ist und mit großer Sicherheit auch in einem größer angelegten Versuch auftreten wird.\\
Des weiteren haben die Tests gezeigt, das bei Verwendung von Einzelbildern der maximale Winkel relativ zur Kamera beträchtlich sinkt. Außerdem kann bei Verwendung eines Videos das Gesicht deutlich kleiner dargestellt werden bis keine Auswertung mehr möglich ist als für die Einzelbilder.

