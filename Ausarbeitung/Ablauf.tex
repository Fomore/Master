\section{Ablauf der Implementierung}
\input{Struktur_Graph}
Da nur eine einzige fest montierte Kamera ohne Zoom eingesetzt wird, muss diese eine entsprechend hohe Auflösung besitzen, damit alle Personen zu erkennen sind. Zur Bestimmung der Blickrichtung sowie Kopfposition und Orientierung wird ein mehrstufiges Verfahren eingesetzt um alle Teilprobleme zu lösen.\\
Am Anfang müssen alle Gesichter, die im aktuellen Frame vorhanden sind, detektiert werden da nur auf diesen eine Berechnung ausgeführt wird.
Dabei machen die relevanten Bereiche nur einen sehr geringen Anteil des gesamten Bildes aus. Dazu wird die MTCNN Face Detection eingesetzt, siehe \autoref{MTCNN}. Dieses Verfahren machte im Vorabtests auf Probebildern einen sehr guten Eindruck und konnte die meisten Gesichtern mit verschieden Größen und Blickrichtungen finden.\\
Für die weiteren Berechnungen muss bekannt sein, welcher Bereich von einem Gesicht im Frame eingenommen wird, um die relevanten Bildausschnitte aufzubereiten. Dabei muss das gesamte Gesicht in der Box sein, weitere Besonderheiten gibt es nicht, da OpenFace einen eigenen Facedetector besitzt. Je nach verwendetem Trainingsdatensatz und darin enthaltener Annotation werden z.B. Kinn und Haaransatz noch als Gesichtsbereich oder schon als außerhalb betrachtet. So geben beiden Methoden (OpenFace und MTCNN-Face) Boxen aus, diese sind in ihren Ausmaßen allerdings nicht identisch. Da die folgende Verarbeitung eine OpenFace-skalierte Box erwartet, hat sich eine Vergrößerung der MTCNN-Face Box um $30\%$ als sinnvoll erwiesen, um Ungenauigkeiten bezüglich der Position und Dimension des Kopfes im Bild entgegen zu wirken.\\
Sind mehrere Gesichter in mehreren Frames des Videos abgebildet, so muss auch eine Identitätszuordnung vorgenommen werden, damit bekannt ist welches Gesicht in Bild 1 welchem in Bild 2 entspricht. Für die Zuordnung reicht es meist aus, jene Box zu wählen, die am ehesten den selben Bildausschnitt repräsentiert wie im vorigen Frame, da sich die Gesichter meist weder groß Bewegen noch sich die einzelnen Boxen der Probanden überlappen.\\
Damit sicher auf allen Gesichter gerechnet werden kann, ist eine semiautomatische Korrektur erforderlich um Falsch-Detektionen zu entfernen und fehlende Boxen der Gesichtern ergänzen zu können. Die gefundenen 5 Landmarks von MTCNN-Face Detection sind für die nachfolgende Berechnung nicht relevant, da sie gerade bei kleinen Gesichtern zu ungenau sind um sie zu verwenden. Daher können alle bisher unternommenen Schritte auch von anderen Verfahren übernommen werden, da es sich hierbei nur um ein Vorverarbeitungsschritt handelt und zur Beschleunigung sowie Stabilität des späteren Berechnung beitragen soll.\\
Damit das Verfahren im nächsten Schritt zuverlässig arbeiten kann, werden alle zu kleinen Bildbereiche hochskaliert, um die Gesichter auf eine Mindestgröße zu bringen, siehe \autoref{scale_Algos}\\
Diese Bildbereiche werden nun von OpenFace weiterverarbeitet um die Landmarks, die signifikanten Punkte eines Gesichtes, zu bestimmen. Durch die vorige Zuordnung der Gesichert kann das Verfahren gezielt auf den einzelnen Person arbeiten und ein entsprechend eingestelltes CLNF verwenden, um bessere Ergebnisse zu erzielen, siehe \autoref{OpenFace}. Außerdem könne alle gefundenen Personen gleichzeitig (parallel) ausgewertet werden. Für dem im nächsten Schritt verwendetem ElSe Algorithmus, muss der Bildausschnitt des Auges in ein Graubild umgewandelt werden, siehe \autoref{Graubild}.\\
Um die Position der Pupille noch exakter zu ermitteln wird ElSe verwendet, da durch eine exakte Bestimmung der Pupillenposition, auch eine genaue Blickrichtungsbestimmung möglich ist, siehe \autoref{ElSe}.\\
Nun wird auf Basis der Landmarks und Kameraparameter die Position und Orientierung der Gesichter sowie die Blickrichtung bestimmt, siehe \autoref{calc_Position}. Diese Ergebnisse können dann von weiteren Anwendungen verwendet werden.